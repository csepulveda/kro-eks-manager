apiVersion: kro.run/v1alpha1
kind: ResourceGraphDefinition
metadata:
  name: ekscluster.kro.run
spec:
  schema:
    apiVersion: v1alpha1
    kind: EKSCluster
    spec:
      name: string
      vpcCIDR: string
      subnetACIDR: string
      subnetBCIDR: string
      version: string
      adminARN: string
    status:
      networkingInfo:
        vpcID: ${clusterVPC.status.vpcID}
        subnetAZA: ${clusterSubnetA.status.subnetID}
        subnetAZB: ${clusterSubnetB.status.subnetID}
      clusterARN: ${cluster.status.ackResourceMetadata.arn}
  resources:
    - id: clusterVPC
      readyWhen:
        - ${clusterVPC.status.state == "available"}
      template:
        apiVersion: ec2.services.k8s.aws/v1alpha1
        kind: VPC
        metadata:
          name: ${schema.spec.name}-vpc
        spec:
          cidrBlocks:
            - ${schema.spec.vpcCIDR}
          enableDNSSupport: true
          enableDNSHostnames: true
    - id: clusterElasticIPAddress
      template:
        apiVersion: ec2.services.k8s.aws/v1alpha1
        kind: ElasticIPAddress
        metadata:
          name: ${schema.spec.name}-eip
        spec: {}
    - id: clusterInternetGateway
      template:
        apiVersion: ec2.services.k8s.aws/v1alpha1
        kind: InternetGateway
        metadata:
          name: ${schema.spec.name}-igw
        spec:
          vpc: ${clusterVPC.status.vpcID}
    - id: clusterRouteTable
      template:
        apiVersion: ec2.services.k8s.aws/v1alpha1
        kind: RouteTable
        metadata:
          name: ${schema.spec.name}-public-route-table
        spec:
          vpcID: ${clusterVPC.status.vpcID}
          routes:
            - destinationCIDRBlock: 0.0.0.0/0
              gatewayID: ${clusterInternetGateway.status.internetGatewayID}
    - id: clusterSubnetA
      readyWhen:
        - ${clusterSubnetA.status.state == "available"}
      template:
        apiVersion: ec2.services.k8s.aws/v1alpha1
        kind: Subnet
        metadata:
          name: ${schema.spec.name}-public-subnet1
        spec:
          availabilityZone: us-west-2a
          cidrBlock: ${schema.spec.subnetACIDR}
          vpcID: ${clusterVPC.status.vpcID}
          routeTables:
            - ${clusterRouteTable.status.routeTableID}
          mapPublicIPOnLaunch: true
    - id: clusterSubnetB
      template:
        apiVersion: ec2.services.k8s.aws/v1alpha1
        kind: Subnet
        metadata:
          name: ${schema.spec.name}-public-subnet2
        spec:
          availabilityZone: us-west-2b
          cidrBlock: ${schema.spec.subnetBCIDR}
          vpcID: ${clusterVPC.status.vpcID}
          routeTables:
            - ${clusterRouteTable.status.routeTableID}
          mapPublicIPOnLaunch: true
    - id: clusterNATGateway
      template:
        apiVersion: ec2.services.k8s.aws/v1alpha1
        kind: NATGateway
        metadata:
          name: ${schema.spec.name}-natgateway1
        spec:
          subnetID: ${clusterSubnetB.status.subnetID}
          allocationID: ${clusterElasticIPAddress.status.allocationID}
    - id: clusterRole
      template:
        apiVersion: iam.services.k8s.aws/v1alpha1
        kind: Role
        metadata:
          name: ${schema.spec.name}-role
        spec:
          name: ${schema.spec.name}-role
          description: "${schema.spec.name} created cluster role"
          policies:
            - arn:aws:iam::aws:policy/AmazonEKSClusterPolicy
          assumeRolePolicyDocument: |
            {
              "Version": "2012-10-17",
              "Statement": [
                {
                  "Effect": "Allow",
                  "Principal": {
                    "Service": "eks.amazonaws.com"
                  },
                  "Action": "sts:AssumeRole"
                }
              ]
            }
    - id: clusterNodeRole
      template:
        apiVersion: iam.services.k8s.aws/v1alpha1
        kind: Role
        metadata:
          name: ${schema.spec.name}-node-role
        spec:
          name: ${schema.spec.name}-node-role
          description: "${schema.spec.name} created cluster node role"
          policies:
            - arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
            - arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly
            - arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
          assumeRolePolicyDocument: |
            {
              "Version": "2012-10-17",
              "Statement": [
                {
                  "Effect": "Allow",
                  "Principal": {
                    "Service": "ec2.amazonaws.com"
                  },
                  "Action": "sts:AssumeRole"
                }
              ]
            }
    - id: clusterAdminRole
      template:
        apiVersion: iam.services.k8s.aws/v1alpha1
        kind: Role
        metadata:
          name: ${schema.spec.name}-cluster-pia-role
        spec:
          name: ${schema.spec.name}-cluster-pia-role
          description: "${schema.spec.name} created cluster admin pia role"
          policies:
            - arn:aws:iam::aws:policy/AdministratorAccess
          assumeRolePolicyDocument: |
            {
                "Version": "2012-10-17",
                "Statement": [
                    {
                        "Sid": "AllowEksAuthToAssumeRoleForPodIdentity",
                        "Effect": "Allow",
                        "Principal": {
                            "Service": "pods.eks.amazonaws.com"
                        },
                        "Action": [
                            "sts:AssumeRole",
                            "sts:TagSession"
                        ]
                    }
                ]
            }
    - id: cluster
      readyWhen:
        - ${cluster.status.status == "ACTIVE"}
      template:
        apiVersion: eks.services.k8s.aws/v1alpha1
        kind: Cluster
        metadata:
          name: ${schema.spec.name}
        spec:
          name: ${schema.spec.name}
          accessConfig:
            authenticationMode: API_AND_CONFIG_MAP
            bootstrapClusterCreatorAdminPermissions: true
          roleARN: ${clusterRole.status.ackResourceMetadata.arn}
          version: ${schema.spec.version}
          resourcesVPCConfig:
            endpointPrivateAccess: false
            endpointPublicAccess: true
            subnetIDs:
              - ${clusterSubnetA.status.subnetID}
              - ${clusterSubnetB.status.subnetID}
    - id: eksAddonCoreDNS
      template:
        apiVersion: eks.services.k8s.aws/v1alpha1
        kind: Addon
        metadata:
          name: ${schema.spec.name}-coredns
        spec:
          name: coredns
          clusterName: ${cluster.spec.name}
    - id: eksAddonPodIdentityAgent
      template:
        apiVersion: eks.services.k8s.aws/v1alpha1
        kind: Addon
        metadata:
          name: ${schema.spec.name}-pod-identity-agent
        spec:
          name: eks-pod-identity-agent
          clusterName: ${cluster.spec.name}
    - id: eksAddonKubeProxy
      template:
        apiVersion: eks.services.k8s.aws/v1alpha1
        kind: Addon
        metadata:
          name: kube-proxy
        spec:
          name: kube-proxy
          clusterName: ${cluster.spec.name}
    - id: eksAddonVpcCni
      template:
        apiVersion: eks.services.k8s.aws/v1alpha1
        kind: Addon
        metadata:
          name: ${schema.spec.name}-vpc-cni
        spec:
          name: vpc-cni
          clusterName: ${cluster.spec.name}
    - id: clusterNodeGroup
      template:
        apiVersion: eks.services.k8s.aws/v1alpha1
        kind: Nodegroup
        metadata:
          name: ${schema.spec.name}-ng
        spec:
          name: karpenter-ng
          clusterName: ${cluster.spec.name}
          nodeRole: ${clusterNodeRole.status.ackResourceMetadata.arn}
          subnets:
            - ${clusterSubnetA.status.subnetID}
            - ${clusterSubnetB.status.subnetID}
          diskSize: 20
          amiType: BOTTLEROCKET_x86_64
          instanceTypes:
            - t2.medium
          labels:
            karpenter.sh/controller: "true"
          scalingConfig:
            minSize: 1
            maxSize: 10
            desiredSize: 1
          updateConfig:
            maxUnavailable: 1
    - id: awsEbsCsiAddon
      template:
        apiVersion: eks.services.k8s.aws/v1alpha1
        kind: Addon
        metadata:
          name: ${schema.spec.name}aws-ebs-csi-driver
        spec:
          name: aws-ebs-csi-driver
          clusterName: ${cluster.spec.name}
          resolveConflicts: OVERWRITE
          serviceAccountRoleARN: ${ebsCsiRole.status.ackResourceMetadata.arn}
    - id: ebsCsiRole
      template:
        apiVersion: iam.services.k8s.aws/v1alpha1
        kind: Role
        metadata:
          name: ${schema.spec.name}-ebs-csi-sa-role
        spec:
          name: ebs-csi-sa-role
          description: Role for EBS CSI driver
          policies:
            - arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy
          assumeRolePolicyDocument: |
            {
              "Version": "2012-10-17",
              "Statement": [
                {
                  "Effect": "Allow",
                  "Principal": {
                    "Service": "pods.eks.amazonaws.com"
                  },
                  "Action": [
                    "sts:AssumeRole",
                    "sts:TagSession"
                  ]
                }
              ]
            }
    - id: argoClusterSecret
      template:
        apiVersion: v1
        kind: Secret
        metadata:
          name: ${schema.spec.name}
          namespace: argocd
          labels:
            argocd.argoproj.io/secret-type: cluster
        type: Opaque
        # stringData:
        #   name: ${schema.spec.name}
        #   server: ${cluster.status.endpoint}
        #   config: |
        #     {
        #       "execProviderConfig": {
        #         "apiVersion": "client.authentication.k8s.io/v1beta1",
        #         "command": "argocd-k8s-auth",
        #         "args": [
        #           "aws", "eks", "token",
        #           "--cluster-name", "${schema.spec.name}"
        #         ]
        #       },
        #       "tlsClientConfig": {
        #         "insecure": false,
        #         "caData": "${cluster.status.certificateAuthority.data}"
        #       }
        #     }
    - id: nginxApp
      template:
        apiVersion: argoproj.io/v1alpha1
        kind: Application
        metadata:
          name: ${schema.spec.name}-testnginx
          namespace: argocd
        spec:
          destination:
            server: ${cluster.status.endpoint}
            namespace: default
          source:
            repoURL: https://github.com/csepulveda/argo-sample-app
            targetRevision: main
            path: .
          project: default
          syncPolicy:
            automated:
              prune: true
              selfHeal: true
